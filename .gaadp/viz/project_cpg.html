<!DOCTYPE html>
<html>
<head>
    <title>GAADP Knowledge Graph</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        body { margin: 0; font-family: Arial, sans-serif; }
        #graph { width: 100vw; height: 100vh; }
        .node { cursor: pointer; }
        .node text { font-size: 10px; }
        .link { stroke-opacity: 0.6; }
        .tooltip {
            position: absolute;
            background: white;
            border: 1px solid #ccc;
            padding: 10px;
            border-radius: 4px;
            max-width: 300px;
        }
    </style>
</head>
<body>
    <div id="graph"></div>
    <script>
        const data = {"nodes": [{"id": "code_-3305019123490435745", "type": "CODE", "status": "PENDING", "content_preview": "\"\"\"\nTEST GENERATOR AGENT\nGenerates test cases from SPEC and CODE nodes.\n\"\"\"\nimport json\nimport uuid\n", "color": "#96CEB4", "metadata": {"file_path": "agents/test_generator.py", "language": "python", "imports": ["json", "uuid", "typing", "agents.base_agent", "core.ontology", "infrastructure.graph_db"], "classes": ["RealTestGenerator"], "functions": ["generate_tests_for_verified_code", "process", "_build_system_prompt", "_build_user_prompt", "_parse_test_response", "_generate_test_path"]}}, {"id": "phantom_import_json", "type": "CODE", "status": "COMPLETE", "content_preview": "json", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_uuid", "type": "CODE", "status": "COMPLETE", "content_preview": "uuid", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_typing", "type": "CODE", "status": "COMPLETE", "content_preview": "typing", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_agents.base_agent", "type": "CODE", "status": "COMPLETE", "content_preview": "agents.base_agent", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_core.ontology", "type": "CODE", "status": "COMPLETE", "content_preview": "core.ontology", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_infrastructure.graph_db", "type": "CODE", "status": "COMPLETE", "content_preview": "infrastructure.graph_db", "color": "#96CEB4", "metadata": {}}, {"id": "class_RealTestGenerator", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealTestGenerator(BaseAgent):\n    \"\"\"\n    Generates pytest test cases from specifications and ", "color": "#9B59B6", "metadata": {"name": "RealTestGenerator", "lineno": 14, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealTestGenerator(BaseAgent):\n    \"\"\"\n    Generates pytest test cases from specifications and code.\n\n    Flow:\n    1. Reads SPEC node for expected behavior\n    2. Reads CODE node for implementation details\n    3. Generates comprehensive pytest test cases\n    4. Creates TEST node linked to both SPEC and CODE\n    \"\"\"\n\n    async def process(self, context: Dict) -> Dict:\n        \"\"\"\n        Generate tests for a SPEC/CODE pair.\n\n        Args:\n            context: {\n                'spec_node': {'id': str, 'content': str},\n                'code_node': {'id': str, 'content': str},  # Optional\n                'test_types': ['unit', 'integration', 'edge_cases']  # Optional\n            }\n\n        Returns:\n            {\n                'type': 'TEST',\n                'content': str,  # pytest code\n                'metadata': {\n                    'spec_id': str,\n                    'code_id': str,\n                    'test_count': int,\n                    'file_path': str\n                }\n            }\n        \"\"\"\n        spec_node = context.get('spec_node', context.get('nodes', [{}])[0])\n        code_node = context.get('code_node', {})\n        test_types = context.get('test_types', ['unit', 'edge_cases'])\n\n        spec_content = spec_node.get('content', '')\n        code_content = code_node.get('content', '')\n\n        system_prompt = self._build_system_prompt()\n        user_prompt = self._build_user_prompt(spec_content, code_content, test_types)\n\n        # Get tools for file reading if needed\n        tools_schema = self.get_tools_schema()\n\n        raw_response = self.gateway.call_model(\n            role=\"BUILDER\",  # Use builder model for code generation\n            system_prompt=system_prompt,\n            user_context=user_prompt,\n            tools=tools_schema if tools_schema else None\n        )\n\n        # Parse response\n        result = self._parse_test_response(raw_response)\n\n        # Add metadata\n        result['type'] = NodeType.TEST.value\n        result['status'] = NodeStatus.PENDING.value\n        result['metadata'] = {\n            'spec_id': spec_node.get('id', 'unknown'),\n            'code_id': code_node.get('id', 'unknown'),\n            'test_types': test_types,\n            'file_path': self._generate_test_path(spec_node.get('id', 'test'))\n        }\n\n        return result\n\n    def _build_system_prompt(self) -> str:\n        \"\"\"Build system prompt for test generation.\"\"\"\n        return \"\"\"You are a Test Engineer specializing in pytest test generation.\n\nYour task is to generate comprehensive, high-quality pytest test cases.\n\nGuidelines:\n1. Use pytest fixtures for setup/teardown\n2. Include docstrings explaining each test\n3. Cover: happy path, edge cases, error cases, boundary conditions\n4. Use descriptive test names: test_<function>_<scenario>_<expected>\n5. Include type hints\n6. Use pytest.raises for exception testing\n7. Use parametrize for multiple test cases\n\nOutput Format:\nReturn ONLY valid Python code that can be run with pytest.\nInclude necessary imports at the top.\nDo not include any markdown or explanations outside the code.\n\"\"\"\n\n    def _build_user_prompt(\n        self,\n        spec_content: str,\n        code_content: str,\n        test_types: List[str]\n    ) -> str:\n        \"\"\"Build user prompt with spec and code.\"\"\"\n        prompt = f\"\"\"Generate pytest tests for the following:\n\nSPECIFICATION:\n{spec_content}\n\n\"\"\"\n        if code_content:\n            prompt += f\"\"\"IMPLEMENTATION:\n{code_content}\n\n\"\"\"\n\n        prompt += f\"\"\"TEST TYPES TO GENERATE: {', '.join(test_types)}\n\nGenerate comprehensive pytest test cases covering all specified types.\nReturn only valid Python code.\"\"\"\n\n        return prompt\n\n    def _parse_test_response(self, response: str) -> Dict:\n        \"\"\"Parse LLM response to extract test code.\"\"\"\n        # Try to extract code from markdown if present\n        if '```python' in response:\n            start = response.find('```python') + 9\n            end = response.find('```', start)\n            if end > start:\n                response = response[start:end].strip()\n        elif '```' in response:\n            start = response.find('```') + 3\n            end = response.find('```', start)\n            if end > start:\n                response = response[start:end].strip()\n\n        return {\n            'content': response.strip(),\n            'test_count': response.count('def test_')\n        }\n\n    def _generate_test_path(self, spec_id: str) -> str:\n        \"\"\"Generate test file path from spec ID.\"\"\"\n        clean_id = spec_id.replace('spec_', '').replace('-', '_')[:20]\n        return f\"tests/test_{clean_id}.py\""}}, {"id": "function_generate_tests_for_verified_code", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "async def generate_tests_for_verified_code(\n    db: GraphDB,\n    test_generator: RealTestGenerator\n)", "color": "#3498DB", "metadata": {"name": "generate_tests_for_verified_code", "lineno": 155, "col_offset": 0, "is_async": true, "args": ["db", "test_generator"], "content": "async def generate_tests_for_verified_code(\n    db: GraphDB,\n    test_generator: RealTestGenerator\n) -> List[str]:\n    \"\"\"\n    Generate tests for all verified CODE nodes that don't have tests.\n\n    Args:\n        db: Graph database\n        test_generator: TestGenerator agent instance\n\n    Returns:\n        List of created TEST node IDs\n    \"\"\"\n    created_tests = []\n\n    for node_id, data in db.graph.nodes(data=True):\n        if data.get('type') != NodeType.CODE.value:\n            continue\n        if data.get('status') != NodeStatus.VERIFIED.value:\n            continue\n\n        # Check if this CODE already has tests\n        has_tests = any(\n            db.graph.nodes[pred].get('type') == NodeType.TEST.value\n            for pred in db.graph.predecessors(node_id)\n        )\n        if has_tests:\n            continue\n\n        # Find the SPEC this CODE implements\n        spec_id = None\n        spec_content = \"\"\n        for _, target, edge_data in db.graph.out_edges(node_id, data=True):\n            if edge_data.get('type') == 'IMPLEMENTS':\n                spec_id = target\n                spec_content = db.graph.nodes[target].get('content', '')\n                break\n\n        if not spec_id:\n            continue\n\n        # Generate tests\n        context = {\n            'spec_node': {'id': spec_id, 'content': spec_content},\n            'code_node': {'id': node_id, 'content': data.get('content', '')}\n        }\n\n        try:\n            result = await test_generator.process(context)\n\n            # Create TEST node\n            test_id = f\"test_{uuid.uuid4().hex[:8]}\"\n            db.add_node(\n                test_id,\n                NodeType.TEST,\n                result['content'],\n                metadata=result['metadata']\n            )\n\n            # Link TEST to CODE\n            db.add_edge(\n                test_id, node_id,\n                'VERIFIES',\n                signed_by=\"test_generator\",\n                signature=f\"auto_{uuid.uuid4().hex[:8]}\"\n            )\n\n            created_tests.append(test_id)\n\n        except Exception as e:\n            print(f\"Failed to generate tests for {node_id}: {e}\")\n\n    return created_tests"}}, {"id": "function_process", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "async def process(self, context: Dict) -> Dict:\n        # Analyze prompt templates\n        templates", "color": "#3498DB", "metadata": {"name": "process", "lineno": 124, "col_offset": 4, "is_async": true, "args": ["self", "context"], "content": "async def process(self, context: Dict) -> Dict:\n        # Analyze prompt templates\n        templates = self.templates\n        analysis = {}\n\n        for template_name, template_data in templates.items():\n            if isinstance(template_data, dict) and 'instruction' in template_data:\n                instruction = template_data['instruction']\n                analysis[template_name] = {\n                    \"char_count\": len(instruction),\n                    \"estimated_tokens\": len(instruction) // 4,\n                    \"has_output_schema\": 'output_schema' in template_data,\n                    \"placeholder_count\": instruction.count('{')\n                }\n\n        # Calculate total prompt budget\n        total_tokens = sum(a['estimated_tokens'] for a in analysis.values())\n\n        return {\n            \"verdict\": \"OPTIMIZED\" if total_tokens < 2000 else \"REVIEW_NEEDED\",\n            \"template_analysis\": analysis,\n            \"total_estimated_tokens\": total_tokens,\n            \"recommendations\": self._generate_recommendations(analysis)\n        }"}}, {"id": "function__build_system_prompt", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _build_system_prompt(self) -> str:\n        \"\"\"Build system prompt for test generation.\"\"\"\n      ", "color": "#3498DB", "metadata": {"name": "_build_system_prompt", "lineno": 83, "col_offset": 4, "is_async": false, "args": ["self"], "content": "def _build_system_prompt(self) -> str:\n        \"\"\"Build system prompt for test generation.\"\"\"\n        return \"\"\"You are a Test Engineer specializing in pytest test generation.\n\nYour task is to generate comprehensive, high-quality pytest test cases.\n\nGuidelines:\n1. Use pytest fixtures for setup/teardown\n2. Include docstrings explaining each test\n3. Cover: happy path, edge cases, error cases, boundary conditions\n4. Use descriptive test names: test_<function>_<scenario>_<expected>\n5. Include type hints\n6. Use pytest.raises for exception testing\n7. Use parametrize for multiple test cases\n\nOutput Format:\nReturn ONLY valid Python code that can be run with pytest.\nInclude necessary imports at the top.\nDo not include any markdown or explanations outside the code.\n\"\"\""}}, {"id": "function__build_user_prompt", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _build_user_prompt(\n        self,\n        spec_content: str,\n        code_content: str,\n        ", "color": "#3498DB", "metadata": {"name": "_build_user_prompt", "lineno": 104, "col_offset": 4, "is_async": false, "args": ["self", "spec_content", "code_content", "test_types"], "content": "def _build_user_prompt(\n        self,\n        spec_content: str,\n        code_content: str,\n        test_types: List[str]\n    ) -> str:\n        \"\"\"Build user prompt with spec and code.\"\"\"\n        prompt = f\"\"\"Generate pytest tests for the following:\n\nSPECIFICATION:\n{spec_content}\n\n\"\"\"\n        if code_content:\n            prompt += f\"\"\"IMPLEMENTATION:\n{code_content}\n\n\"\"\"\n\n        prompt += f\"\"\"TEST TYPES TO GENERATE: {', '.join(test_types)}\n\nGenerate comprehensive pytest test cases covering all specified types.\nReturn only valid Python code.\"\"\"\n\n        return prompt"}}, {"id": "function__parse_test_response", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _parse_test_response(self, response: str) -> Dict:\n        \"\"\"Parse LLM response to extract test", "color": "#3498DB", "metadata": {"name": "_parse_test_response", "lineno": 130, "col_offset": 4, "is_async": false, "args": ["self", "response"], "content": "def _parse_test_response(self, response: str) -> Dict:\n        \"\"\"Parse LLM response to extract test code.\"\"\"\n        # Try to extract code from markdown if present\n        if '```python' in response:\n            start = response.find('```python') + 9\n            end = response.find('```', start)\n            if end > start:\n                response = response[start:end].strip()\n        elif '```' in response:\n            start = response.find('```') + 3\n            end = response.find('```', start)\n            if end > start:\n                response = response[start:end].strip()\n\n        return {\n            'content': response.strip(),\n            'test_count': response.count('def test_')\n        }"}}, {"id": "function__generate_test_path", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _generate_test_path(self, spec_id: str) -> str:\n        \"\"\"Generate test file path from spec ID.", "color": "#3498DB", "metadata": {"name": "_generate_test_path", "lineno": 149, "col_offset": 4, "is_async": false, "args": ["self", "spec_id"], "content": "def _generate_test_path(self, spec_id: str) -> str:\n        \"\"\"Generate test file path from spec ID.\"\"\"\n        clean_id = spec_id.replace('spec_', '').replace('-', '_')[:20]\n        return f\"tests/test_{clean_id}.py\""}}, {"id": "call_nodes", "type": "CALL", "status": "COMPLETE", "content_preview": "nodes", "color": "#E67E22", "metadata": {"lineno": 94, "col_offset": 26, "args_count": 0}}, {"id": "call_get", "type": "CALL", "status": "COMPLETE", "content_preview": "get", "color": "#E67E22", "metadata": {"lineno": 82, "col_offset": 16, "args_count": 1}}, {"id": "call__build_system_prompt", "type": "CALL", "status": "COMPLETE", "content_preview": "_build_system_prompt", "color": "#E67E22", "metadata": {"lineno": 55, "col_offset": 24, "args_count": 0}}, {"id": "call__build_user_prompt", "type": "CALL", "status": "COMPLETE", "content_preview": "_build_user_prompt", "color": "#E67E22", "metadata": {"lineno": 56, "col_offset": 22, "args_count": 3}}, {"id": "call_get_tools_schema", "type": "CALL", "status": "COMPLETE", "content_preview": "get_tools_schema", "color": "#E67E22", "metadata": {"lineno": 358, "col_offset": 23, "args_count": 0}}, {"id": "call_call_model", "type": "CALL", "status": "COMPLETE", "content_preview": "call_model", "color": "#E67E22", "metadata": {"lineno": 365, "col_offset": 27, "args_count": 0}}, {"id": "call__parse_test_response", "type": "CALL", "status": "COMPLETE", "content_preview": "_parse_test_response", "color": "#E67E22", "metadata": {"lineno": 69, "col_offset": 17, "args_count": 1}}, {"id": "call_any", "type": "CALL", "status": "COMPLETE", "content_preview": "any", "color": "#E67E22", "metadata": {"lineno": 81, "col_offset": 24, "args_count": 1}}, {"id": "call_out_edges", "type": "CALL", "status": "COMPLETE", "content_preview": "out_edges", "color": "#E67E22", "metadata": {"lineno": 83, "col_offset": 31, "args_count": 1}}, {"id": "call__generate_test_path", "type": "CALL", "status": "COMPLETE", "content_preview": "_generate_test_path", "color": "#E67E22", "metadata": {"lineno": 78, "col_offset": 25, "args_count": 1}}, {"id": "call_find", "type": "CALL", "status": "COMPLETE", "content_preview": "find", "color": "#E67E22", "metadata": {"lineno": 139, "col_offset": 20, "args_count": 1}}, {"id": "call_strip", "type": "CALL", "status": "COMPLETE", "content_preview": "strip", "color": "#E67E22", "metadata": {"lineno": 138, "col_offset": 27, "args_count": 0}}, {"id": "call_count", "type": "CALL", "status": "COMPLETE", "content_preview": "count", "color": "#E67E22", "metadata": {"lineno": 136, "col_offset": 41, "args_count": 1}}, {"id": "call_replace", "type": "CALL", "status": "COMPLETE", "content_preview": "replace", "color": "#E67E22", "metadata": {"lineno": 151, "col_offset": 19, "args_count": 2}}, {"id": "call_add_node", "type": "CALL", "status": "COMPLETE", "content_preview": "add_node", "color": "#E67E22", "metadata": {"lineno": 208, "col_offset": 12, "args_count": 3}}, {"id": "call_add_edge", "type": "CALL", "status": "COMPLETE", "content_preview": "add_edge", "color": "#E67E22", "metadata": {"lineno": 216, "col_offset": 12, "args_count": 3}}, {"id": "call_append", "type": "CALL", "status": "COMPLETE", "content_preview": "append", "color": "#E67E22", "metadata": {"lineno": 73, "col_offset": 20, "args_count": 1}}, {"id": "call_join", "type": "CALL", "status": "COMPLETE", "content_preview": "join", "color": "#E67E22", "metadata": {"lineno": 397, "col_offset": 45, "args_count": 1}}, {"id": "call_process", "type": "CALL", "status": "COMPLETE", "content_preview": "process", "color": "#E67E22", "metadata": {"lineno": 204, "col_offset": 27, "args_count": 1}}, {"id": "call_print", "type": "CALL", "status": "COMPLETE", "content_preview": "print", "color": "#E67E22", "metadata": {"lineno": 166, "col_offset": 8, "args_count": 1}}, {"id": "call_predecessors", "type": "CALL", "status": "COMPLETE", "content_preview": "predecessors", "color": "#E67E22", "metadata": {"lineno": 180, "col_offset": 24, "args_count": 1}}, {"id": "call_uuid4", "type": "CALL", "status": "COMPLETE", "content_preview": "uuid4", "color": "#E67E22", "metadata": {"lineno": 220, "col_offset": 34, "args_count": 0}}, {"id": "code_-6102688374110437732", "type": "CODE", "status": "PENDING", "content_preview": "\"\"\"\nBASE AGENT (Hardened with MCP & RBAC)\nIntegrates Runtime Signing, File Locks, LLM Gateway, and T", "color": "#96CEB4", "metadata": {"file_path": "agents/base_agent.py", "language": "python", "imports": ["json", "logging", "yaml", "os", "time", "abc", "typing", "cryptography.hazmat.primitives.asymmetric", "cryptography.hazmat.primitives", "infrastructure.llm_gateway", "core.ontology", "infrastructure.mcp_hub", "re"], "classes": ["BaseAgent"], "functions": ["__init__", "_save_keys", "acquire_lock", "release_lock", "sign_content", "_hydrate_prompt", "_parse_json_response", "get_tools_schema", "check_tool_permission", "execute_tool_calls", "process"]}}, {"id": "phantom_import_logging", "type": "CODE", "status": "COMPLETE", "content_preview": "logging", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_yaml", "type": "CODE", "status": "COMPLETE", "content_preview": "yaml", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_os", "type": "CODE", "status": "COMPLETE", "content_preview": "os", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_time", "type": "CODE", "status": "COMPLETE", "content_preview": "time", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_abc", "type": "CODE", "status": "COMPLETE", "content_preview": "abc", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_cryptography.hazmat.primitives.asymmetric", "type": "CODE", "status": "COMPLETE", "content_preview": "cryptography.hazmat.primitives.asymmetric", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_cryptography.hazmat.primitives", "type": "CODE", "status": "COMPLETE", "content_preview": "cryptography.hazmat.primitives", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_infrastructure.llm_gateway", "type": "CODE", "status": "COMPLETE", "content_preview": "infrastructure.llm_gateway", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_infrastructure.mcp_hub", "type": "CODE", "status": "COMPLETE", "content_preview": "infrastructure.mcp_hub", "color": "#96CEB4", "metadata": {}}, {"id": "phantom_import_re", "type": "CODE", "status": "COMPLETE", "content_preview": "re", "color": "#96CEB4", "metadata": {}}, {"id": "class_BaseAgent", "type": "CLASS", "status": "COMPLETE", "content_preview": "class BaseAgent(ABC):\n    def __init__(\n        self,\n        agent_id: str,\n        role: AgentRole", "color": "#9B59B6", "metadata": {"name": "BaseAgent", "lineno": 24, "col_offset": 0, "bases": ["ABC"], "content": "class BaseAgent(ABC):\n    def __init__(\n        self,\n        agent_id: str,\n        role: AgentRole,\n        graph_db,\n        mcp_hub: Optional[\"MCPHub\"] = None\n    ):\n        self.agent_id = agent_id\n        self.role = role\n        self.graph_db = graph_db\n        self.mcp_hub = mcp_hub\n        self.gateway = LLMGateway()\n        self.logger = logging.getLogger(f\"Agent.{role}.{agent_id}\")\n        self._private_key = ed25519.Ed25519PrivateKey.generate()\n        self._public_key = self._private_key.public_key()\n        self._save_keys()\n\n        with open(\".blueprint/prompt_templates.yaml\", \"r\") as f:\n            self.templates = yaml.safe_load(f)\n        with open(\".blueprint/prime_directives.md\", \"r\") as f:\n            self.directives = f.read()\n\n        # Load topology for RBAC\n        try:\n            with open(\".blueprint/topology_config.yaml\", \"r\") as f:\n                self.topology = yaml.safe_load(f)\n        except FileNotFoundError:\n            self.topology = {\"tool_permissions\": {}}\n\n        # Cache allowed tools for fast lookup\n        self.allowed_tools = (\n            self.topology\n            .get(\"tool_permissions\", {})\n            .get(role.value, {})\n            .get(\"allowed_tools\", [])\n        )\n\n    def _save_keys(self):\n        key_dir = \".gaadp/keys\"\n        os.makedirs(key_dir, exist_ok=True)\n        pub_key_path = f\"{key_dir}/{self.agent_id}.pub\"\n        pem = self._public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        with open(pub_key_path, \"wb\") as f:\n            f.write(pem)\n\n    def acquire_lock(self, node_id: str, timeout: int = 10) -> bool:\n        lock_dir = \".gaadp/locks\"\n        os.makedirs(lock_dir, exist_ok=True)\n        lock_file = f\"{lock_dir}/{node_id}.lock\"\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                with open(lock_file, \"x\") as f:\n                    f.write(self.agent_id)\n                return True\n            except FileExistsError:\n                if os.path.getmtime(lock_file) < time.time() - 30:\n                    os.remove(lock_file)\n                    continue\n                time.sleep(0.1)\n        return False\n\n    def release_lock(self, node_id: str):\n        lock_file = f\".gaadp/locks/{node_id}.lock\"\n        if os.path.exists(lock_file):\n            os.remove(lock_file)\n\n    def sign_content(self, content: Any, previous_hash: str = \"GENESIS\") -> str:\n        \"\"\"\n        Cryptographically signs data linked to history.\n        Creates a Merkle-like chain of custody.\n        \"\"\"\n        payload = {\n            \"content\": content,\n            \"prev_hash\": previous_hash,\n            \"agent_id\": self.agent_id,\n            \"timestamp\": time.time()\n        }\n        data_bytes = json.dumps(payload, sort_keys=True).encode('utf-8')\n        signature = self._private_key.sign(data_bytes).hex()\n        return signature\n\n    def _hydrate_prompt(self, template_id: str, vars: Dict) -> str:\n        template = self.templates.get(template_id, {}).get('instruction', \"\")\n        vars['prime_directives_text'] = self.directives\n        vars['agent_role'] = self.role.value\n        vars['agent_id'] = self.agent_id\n        return template.format(**vars)\n\n    def _parse_json_response(self, text: str) -> Dict:\n        \"\"\"\n        Parse LLM response, handling JSON, Markdown blocks, and mixed content.\n\n        Strategies:\n        1. Direct JSON parse\n        2. Extract from ```json ... ``` blocks\n        3. Find first outer brace { ... }\n        4. Raise ValueError with debug output\n\n        Args:\n            text: Raw LLM response\n\n        Returns:\n            Parsed dict\n\n        Raises:\n            ValueError: If no valid JSON can be extracted\n        \"\"\"\n        import re\n\n        if not text or not text.strip():\n            raise ValueError(\"Empty response from LLM\")\n\n        text = text.strip()\n\n        # 1. Try direct parsing\n        try:\n            return json.loads(text)\n        except json.JSONDecodeError:\n            pass\n\n        # 2. Try extracting from ```json ... ``` blocks\n        match = re.search(r\"```(?:json)?\\s*(.*?)```\", text, re.DOTALL)\n        if match:\n            try:\n                return json.loads(match.group(1))\n            except json.JSONDecodeError:\n                pass\n\n        # 3. Try finding the first outer brace { ... }\n        match = re.search(r\"(\\{.*\\})\", text, re.DOTALL)\n        if match:\n            try:\n                return json.loads(match.group(1))\n            except json.JSONDecodeError:\n                pass\n\n        # 4. Debug: Print what failed and return a sensible default\n        print(f\"\u274c JSON PARSE FAILED. Raw Output:\\n{text[:500]}...\\n\")\n        self.logger.warning(\"LLM returned non-JSON response, wrapping as content\")\n        return {\n            \"content\": text,\n            \"verdict\": \"FAIL\",\n            \"critique\": \"LLM failed to produce valid JSON output - response was conversational text\",\n            \"parse_error\": True\n        }\n\n    def get_tools_schema(self) -> List[Dict]:\n        \"\"\"Get filtered tool schemas for this agent's role.\"\"\"\n        if self.mcp_hub:\n            return self.mcp_hub.get_tools_for_role(self.role.value)\n        return []\n\n    def check_tool_permission(self, tool_name: str) -> bool:\n        \"\"\"Check if this agent can use a specific tool.\"\"\"\n        return tool_name in self.allowed_tools\n\n    async def execute_tool_calls(self, response: Dict) -> str:\n        \"\"\"\n        Execute tool calls from LLM response with permission checking.\n\n        Args:\n            response: Parsed LLM response containing tool_calls\n\n        Returns:\n            Concatenated results from all tool executions\n\n        Supports multiple tool call formats:\n            - OpenAI/Anthropic API format: {\"function\": {\"name\": \"...\", \"arguments\": \"...\"}}\n            - Simple format: {\"name\": \"...\", \"input\": {...}}\n        \"\"\"\n        if not self.mcp_hub:\n            return \"No MCP Hub configured\"\n\n        tool_calls = response.get('tool_calls', [])\n        results_log = []\n\n        for call in tool_calls:\n            # Support both API format and simple format\n            if 'function' in call:\n                # OpenAI/Anthropic API format\n                func_name = call['function']['name']\n                raw_args = call['function'].get('arguments', '{}')\n            else:\n                # Simple format (e.g., from ManualProvider)\n                func_name = call.get('name', 'unknown')\n                raw_args = call.get('input', {})\n\n            # Handle arguments: can be string (needs parsing) or dict (use directly)\n            if isinstance(raw_args, str):\n                try:\n                    args = json.loads(raw_args)\n                except json.JSONDecodeError:\n                    results_log.append(f\"Tool '{func_name}' Failed: Invalid JSON arguments: {raw_args}\")\n                    continue\n            elif isinstance(raw_args, dict):\n                args = raw_args\n            else:\n                args = {}\n\n            # SECURITY CHECK\n            if not self.check_tool_permission(func_name):\n                denial_msg = (\n                    f\"\u26d4 SECURITY ALERT: Agent '{self.agent_id}' ({self.role.value}) \"\n                    f\"attempted to use forbidden tool '{func_name}'\"\n                )\n                self.logger.warning(denial_msg)\n                results_log.append(denial_msg)\n                continue\n\n            try:\n                self.logger.info(f\"\ud83d\udee0\ufe0f Executing Tool: {func_name}\")\n                result = await self.mcp_hub.execute_tool(\n                    func_name, args, role_name=self.role.value\n                )\n                results_log.append(f\"Tool '{func_name}' Output: {str(result)}\")\n            except PermissionError as e:\n                results_log.append(f\"\u26d4 Permission Denied: {str(e)}\")\n            except Exception as e:\n                results_log.append(f\"Tool '{func_name}' Failed: {str(e)}\")\n\n        return \"\\n\".join(results_log)\n\n    @abstractmethod\n    async def process(self, context: Dict) -> Dict:\n        pass"}}, {"id": "function___init__", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def __init__(\n        self,\n        agent_id: str,\n        role: AgentRole,\n        graph_db,\n      ", "color": "#3498DB", "metadata": {"name": "__init__", "lineno": 25, "col_offset": 4, "is_async": false, "args": ["self", "agent_id", "role", "graph_db", "mcp_hub"], "content": "def __init__(\n        self,\n        agent_id: str,\n        role: AgentRole,\n        graph_db,\n        mcp_hub: Optional[\"MCPHub\"] = None\n    ):\n        self.agent_id = agent_id\n        self.role = role\n        self.graph_db = graph_db\n        self.mcp_hub = mcp_hub\n        self.gateway = LLMGateway()\n        self.logger = logging.getLogger(f\"Agent.{role}.{agent_id}\")\n        self._private_key = ed25519.Ed25519PrivateKey.generate()\n        self._public_key = self._private_key.public_key()\n        self._save_keys()\n\n        with open(\".blueprint/prompt_templates.yaml\", \"r\") as f:\n            self.templates = yaml.safe_load(f)\n        with open(\".blueprint/prime_directives.md\", \"r\") as f:\n            self.directives = f.read()\n\n        # Load topology for RBAC\n        try:\n            with open(\".blueprint/topology_config.yaml\", \"r\") as f:\n                self.topology = yaml.safe_load(f)\n        except FileNotFoundError:\n            self.topology = {\"tool_permissions\": {}}\n\n        # Cache allowed tools for fast lookup\n        self.allowed_tools = (\n            self.topology\n            .get(\"tool_permissions\", {})\n            .get(role.value, {})\n            .get(\"allowed_tools\", [])\n        )"}}, {"id": "function__save_keys", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _save_keys(self):\n        key_dir = \".gaadp/keys\"\n        os.makedirs(key_dir, exist_ok=True)\n  ", "color": "#3498DB", "metadata": {"name": "_save_keys", "lineno": 62, "col_offset": 4, "is_async": false, "args": ["self"], "content": "def _save_keys(self):\n        key_dir = \".gaadp/keys\"\n        os.makedirs(key_dir, exist_ok=True)\n        pub_key_path = f\"{key_dir}/{self.agent_id}.pub\"\n        pem = self._public_key.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        with open(pub_key_path, \"wb\") as f:\n            f.write(pem)"}}, {"id": "function_acquire_lock", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def acquire_lock(self, node_id: str, timeout: int = 10) -> bool:\n        lock_dir = \".gaadp/locks\"\n ", "color": "#3498DB", "metadata": {"name": "acquire_lock", "lineno": 73, "col_offset": 4, "is_async": false, "args": ["self", "node_id", "timeout"], "content": "def acquire_lock(self, node_id: str, timeout: int = 10) -> bool:\n        lock_dir = \".gaadp/locks\"\n        os.makedirs(lock_dir, exist_ok=True)\n        lock_file = f\"{lock_dir}/{node_id}.lock\"\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                with open(lock_file, \"x\") as f:\n                    f.write(self.agent_id)\n                return True\n            except FileExistsError:\n                if os.path.getmtime(lock_file) < time.time() - 30:\n                    os.remove(lock_file)\n                    continue\n                time.sleep(0.1)\n        return False"}}, {"id": "function_release_lock", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def release_lock(self, node_id: str):\n        lock_file = f\".gaadp/locks/{node_id}.lock\"\n        if ", "color": "#3498DB", "metadata": {"name": "release_lock", "lineno": 90, "col_offset": 4, "is_async": false, "args": ["self", "node_id"], "content": "def release_lock(self, node_id: str):\n        lock_file = f\".gaadp/locks/{node_id}.lock\"\n        if os.path.exists(lock_file):\n            os.remove(lock_file)"}}, {"id": "function_sign_content", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def sign_content(self, content: Any, previous_hash: str = \"GENESIS\") -> str:\n        \"\"\"\n        Cry", "color": "#3498DB", "metadata": {"name": "sign_content", "lineno": 95, "col_offset": 4, "is_async": false, "args": ["self", "content", "previous_hash"], "content": "def sign_content(self, content: Any, previous_hash: str = \"GENESIS\") -> str:\n        \"\"\"\n        Cryptographically signs data linked to history.\n        Creates a Merkle-like chain of custody.\n        \"\"\"\n        payload = {\n            \"content\": content,\n            \"prev_hash\": previous_hash,\n            \"agent_id\": self.agent_id,\n            \"timestamp\": time.time()\n        }\n        data_bytes = json.dumps(payload, sort_keys=True).encode('utf-8')\n        signature = self._private_key.sign(data_bytes).hex()\n        return signature"}}, {"id": "function__hydrate_prompt", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _hydrate_prompt(self, template_id: str, vars: Dict) -> str:\n        template = self.templates.ge", "color": "#3498DB", "metadata": {"name": "_hydrate_prompt", "lineno": 110, "col_offset": 4, "is_async": false, "args": ["self", "template_id", "vars"], "content": "def _hydrate_prompt(self, template_id: str, vars: Dict) -> str:\n        template = self.templates.get(template_id, {}).get('instruction', \"\")\n        vars['prime_directives_text'] = self.directives\n        vars['agent_role'] = self.role.value\n        vars['agent_id'] = self.agent_id\n        return template.format(**vars)"}}, {"id": "function__parse_json_response", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _parse_json_response(self, text: str) -> Dict:\n        \"\"\"\n        Parse LLM response, handling ", "color": "#3498DB", "metadata": {"name": "_parse_json_response", "lineno": 117, "col_offset": 4, "is_async": false, "args": ["self", "text"], "content": "def _parse_json_response(self, text: str) -> Dict:\n        \"\"\"\n        Parse LLM response, handling JSON, Markdown blocks, and mixed content.\n\n        Strategies:\n        1. Direct JSON parse\n        2. Extract from ```json ... ``` blocks\n        3. Find first outer brace { ... }\n        4. Raise ValueError with debug output\n\n        Args:\n            text: Raw LLM response\n\n        Returns:\n            Parsed dict\n\n        Raises:\n            ValueError: If no valid JSON can be extracted\n        \"\"\"\n        import re\n\n        if not text or not text.strip():\n            raise ValueError(\"Empty response from LLM\")\n\n        text = text.strip()\n\n        # 1. Try direct parsing\n        try:\n            return json.loads(text)\n        except json.JSONDecodeError:\n            pass\n\n        # 2. Try extracting from ```json ... ``` blocks\n        match = re.search(r\"```(?:json)?\\s*(.*?)```\", text, re.DOTALL)\n        if match:\n            try:\n                return json.loads(match.group(1))\n            except json.JSONDecodeError:\n                pass\n\n        # 3. Try finding the first outer brace { ... }\n        match = re.search(r\"(\\{.*\\})\", text, re.DOTALL)\n        if match:\n            try:\n                return json.loads(match.group(1))\n            except json.JSONDecodeError:\n                pass\n\n        # 4. Debug: Print what failed and return a sensible default\n        print(f\"\u274c JSON PARSE FAILED. Raw Output:\\n{text[:500]}...\\n\")\n        self.logger.warning(\"LLM returned non-JSON response, wrapping as content\")\n        return {\n            \"content\": text,\n            \"verdict\": \"FAIL\",\n            \"critique\": \"LLM failed to produce valid JSON output - response was conversational text\",\n            \"parse_error\": True\n        }"}}, {"id": "function_get_tools_schema", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def get_tools_schema(self) -> List[Dict]:\n        \"\"\"Get filtered tool schemas for this agent's role", "color": "#3498DB", "metadata": {"name": "get_tools_schema", "lineno": 175, "col_offset": 4, "is_async": false, "args": ["self"], "content": "def get_tools_schema(self) -> List[Dict]:\n        \"\"\"Get filtered tool schemas for this agent's role.\"\"\"\n        if self.mcp_hub:\n            return self.mcp_hub.get_tools_for_role(self.role.value)\n        return []"}}, {"id": "function_check_tool_permission", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def check_tool_permission(self, tool_name: str) -> bool:\n        \"\"\"Check if this agent can use a sp", "color": "#3498DB", "metadata": {"name": "check_tool_permission", "lineno": 181, "col_offset": 4, "is_async": false, "args": ["self", "tool_name"], "content": "def check_tool_permission(self, tool_name: str) -> bool:\n        \"\"\"Check if this agent can use a specific tool.\"\"\"\n        return tool_name in self.allowed_tools"}}, {"id": "function_execute_tool_calls", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "async def execute_tool_calls(self, response: Dict) -> str:\n        \"\"\"\n        Execute tool calls fr", "color": "#3498DB", "metadata": {"name": "execute_tool_calls", "lineno": 185, "col_offset": 4, "is_async": true, "args": ["self", "response"], "content": "async def execute_tool_calls(self, response: Dict) -> str:\n        \"\"\"\n        Execute tool calls from LLM response with permission checking.\n\n        Args:\n            response: Parsed LLM response containing tool_calls\n\n        Returns:\n            Concatenated results from all tool executions\n\n        Supports multiple tool call formats:\n            - OpenAI/Anthropic API format: {\"function\": {\"name\": \"...\", \"arguments\": \"...\"}}\n            - Simple format: {\"name\": \"...\", \"input\": {...}}\n        \"\"\"\n        if not self.mcp_hub:\n            return \"No MCP Hub configured\"\n\n        tool_calls = response.get('tool_calls', [])\n        results_log = []\n\n        for call in tool_calls:\n            # Support both API format and simple format\n            if 'function' in call:\n                # OpenAI/Anthropic API format\n                func_name = call['function']['name']\n                raw_args = call['function'].get('arguments', '{}')\n            else:\n                # Simple format (e.g., from ManualProvider)\n                func_name = call.get('name', 'unknown')\n                raw_args = call.get('input', {})\n\n            # Handle arguments: can be string (needs parsing) or dict (use directly)\n            if isinstance(raw_args, str):\n                try:\n                    args = json.loads(raw_args)\n                except json.JSONDecodeError:\n                    results_log.append(f\"Tool '{func_name}' Failed: Invalid JSON arguments: {raw_args}\")\n                    continue\n            elif isinstance(raw_args, dict):\n                args = raw_args\n            else:\n                args = {}\n\n            # SECURITY CHECK\n            if not self.check_tool_permission(func_name):\n                denial_msg = (\n                    f\"\u26d4 SECURITY ALERT: Agent '{self.agent_id}' ({self.role.value}) \"\n                    f\"attempted to use forbidden tool '{func_name}'\"\n                )\n                self.logger.warning(denial_msg)\n                results_log.append(denial_msg)\n                continue\n\n            try:\n                self.logger.info(f\"\ud83d\udee0\ufe0f Executing Tool: {func_name}\")\n                result = await self.mcp_hub.execute_tool(\n                    func_name, args, role_name=self.role.value\n                )\n                results_log.append(f\"Tool '{func_name}' Output: {str(result)}\")\n            except PermissionError as e:\n                results_log.append(f\"\u26d4 Permission Denied: {str(e)}\")\n            except Exception as e:\n                results_log.append(f\"Tool '{func_name}' Failed: {str(e)}\")\n\n        return \"\\n\".join(results_log)"}}, {"id": "call_LLMGateway", "type": "CALL", "status": "COMPLETE", "content_preview": "LLMGateway", "color": "#E67E22", "metadata": {"lineno": 36, "col_offset": 23, "args_count": 0}}, {"id": "call_getLogger", "type": "CALL", "status": "COMPLETE", "content_preview": "getLogger", "color": "#E67E22", "metadata": {"lineno": 37, "col_offset": 22, "args_count": 1}}, {"id": "call_generate", "type": "CALL", "status": "COMPLETE", "content_preview": "generate", "color": "#E67E22", "metadata": {"lineno": 38, "col_offset": 28, "args_count": 0}}, {"id": "call_public_key", "type": "CALL", "status": "COMPLETE", "content_preview": "public_key", "color": "#E67E22", "metadata": {"lineno": 39, "col_offset": 27, "args_count": 0}}, {"id": "call__save_keys", "type": "CALL", "status": "COMPLETE", "content_preview": "_save_keys", "color": "#E67E22", "metadata": {"lineno": 40, "col_offset": 8, "args_count": 0}}, {"id": "call_makedirs", "type": "CALL", "status": "COMPLETE", "content_preview": "makedirs", "color": "#E67E22", "metadata": {"lineno": 75, "col_offset": 8, "args_count": 1}}, {"id": "call_public_bytes", "type": "CALL", "status": "COMPLETE", "content_preview": "public_bytes", "color": "#E67E22", "metadata": {"lineno": 66, "col_offset": 14, "args_count": 0}}, {"id": "call_time", "type": "CALL", "status": "COMPLETE", "content_preview": "time", "color": "#E67E22", "metadata": {"lineno": 84, "col_offset": 49, "args_count": 0}}, {"id": "call_exists", "type": "CALL", "status": "COMPLETE", "content_preview": "exists", "color": "#E67E22", "metadata": {"lineno": 92, "col_offset": 11, "args_count": 1}}, {"id": "call_encode", "type": "CALL", "status": "COMPLETE", "content_preview": "encode", "color": "#E67E22", "metadata": {"lineno": 106, "col_offset": 21, "args_count": 1}}, {"id": "call_hex", "type": "CALL", "status": "COMPLETE", "content_preview": "hex", "color": "#E67E22", "metadata": {"lineno": 107, "col_offset": 20, "args_count": 0}}, {"id": "call_format", "type": "CALL", "status": "COMPLETE", "content_preview": "format", "color": "#E67E22", "metadata": {"lineno": 115, "col_offset": 15, "args_count": 0}}, {"id": "call_search", "type": "CALL", "status": "COMPLETE", "content_preview": "search", "color": "#E67E22", "metadata": {"lineno": 158, "col_offset": 16, "args_count": 3}}, {"id": "call_warning", "type": "CALL", "status": "COMPLETE", "content_preview": "warning", "color": "#E67E22", "metadata": {"lineno": 35, "col_offset": 12, "args_count": 1}}, {"id": "call_open", "type": "CALL", "status": "COMPLETE", "content_preview": "open", "color": "#E67E22", "metadata": {"lineno": 80, "col_offset": 21, "args_count": 2}}, {"id": "call_safe_load", "type": "CALL", "status": "COMPLETE", "content_preview": "safe_load", "color": "#E67E22", "metadata": {"lineno": 50, "col_offset": 32, "args_count": 1}}, {"id": "call_read", "type": "CALL", "status": "COMPLETE", "content_preview": "read", "color": "#E67E22", "metadata": {"lineno": 45, "col_offset": 30, "args_count": 0}}, {"id": "call_write", "type": "CALL", "status": "COMPLETE", "content_preview": "write", "color": "#E67E22", "metadata": {"lineno": 81, "col_offset": 20, "args_count": 1}}, {"id": "call_remove", "type": "CALL", "status": "COMPLETE", "content_preview": "remove", "color": "#E67E22", "metadata": {"lineno": 85, "col_offset": 20, "args_count": 1}}, {"id": "call_ValueError", "type": "CALL", "status": "COMPLETE", "content_preview": "ValueError", "color": "#E67E22", "metadata": {"lineno": 139, "col_offset": 18, "args_count": 1}}, {"id": "call_loads", "type": "CALL", "status": "COMPLETE", "content_preview": "loads", "color": "#E67E22", "metadata": {"lineno": 374, "col_offset": 25, "args_count": 1}}, {"id": "call_get_tools_for_role", "type": "CALL", "status": "COMPLETE", "content_preview": "get_tools_for_role", "color": "#E67E22", "metadata": {"lineno": 178, "col_offset": 19, "args_count": 1}}, {"id": "call_isinstance", "type": "CALL", "status": "COMPLETE", "content_preview": "isinstance", "color": "#E67E22", "metadata": {"lineno": 130, "col_offset": 15, "args_count": 2}}, {"id": "call_dumps", "type": "CALL", "status": "COMPLETE", "content_preview": "dumps", "color": "#E67E22", "metadata": {"lineno": 106, "col_offset": 21, "args_count": 1}}, {"id": "call_sign", "type": "CALL", "status": "COMPLETE", "content_preview": "sign", "color": "#E67E22", "metadata": {"lineno": 107, "col_offset": 20, "args_count": 1}}, {"id": "call_check_tool_permission", "type": "CALL", "status": "COMPLETE", "content_preview": "check_tool_permission", "color": "#E67E22", "metadata": {"lineno": 229, "col_offset": 19, "args_count": 1}}, {"id": "call_info", "type": "CALL", "status": "COMPLETE", "content_preview": "info", "color": "#E67E22", "metadata": {"lineno": 394, "col_offset": 20, "args_count": 1}}, {"id": "call_sleep", "type": "CALL", "status": "COMPLETE", "content_preview": "sleep", "color": "#E67E22", "metadata": {"lineno": 87, "col_offset": 16, "args_count": 1}}, {"id": "call_group", "type": "CALL", "status": "COMPLETE", "content_preview": "group", "color": "#E67E22", "metadata": {"lineno": 161, "col_offset": 34, "args_count": 1}}, {"id": "call_execute_tool", "type": "CALL", "status": "COMPLETE", "content_preview": "execute_tool", "color": "#E67E22", "metadata": {"lineno": 240, "col_offset": 31, "args_count": 2}}, {"id": "call_getmtime", "type": "CALL", "status": "COMPLETE", "content_preview": "getmtime", "color": "#E67E22", "metadata": {"lineno": 84, "col_offset": 19, "args_count": 1}}, {"id": "call_str", "type": "CALL", "status": "COMPLETE", "content_preview": "str", "color": "#E67E22", "metadata": {"lineno": 247, "col_offset": 65, "args_count": 1}}, {"id": "code_-5582610556842541187", "type": "CODE", "status": "PENDING", "content_preview": "# Package marker\n", "color": "#96CEB4", "metadata": {"file_path": "agents/__init__.py", "language": "python", "imports": [], "classes": [], "functions": []}}, {"id": "code_1449790842580194339", "type": "CODE", "status": "PENDING", "content_preview": "\"\"\"\nCONCRETE AGENTS\nProduction implementations with optional MCP tool support.\nImplements ReAct (Rea", "color": "#96CEB4", "metadata": {"file_path": "agents/concrete_agents.py", "language": "python", "imports": ["json", "typing", "agents.base_agent", "core.ontology"], "classes": ["RealArchitect", "RealBuilder", "RealVerifier", "RealSocrates"], "functions": ["process", "_build_escalation_prompt"]}}, {"id": "class_RealArchitect", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealArchitect(BaseAgent):\n    \"\"\"\n    Decomposes requirements into atomic specs and plans.\n   ", "color": "#9B59B6", "metadata": {"name": "RealArchitect", "lineno": 15, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealArchitect(BaseAgent):\n    \"\"\"\n    Decomposes requirements into atomic specs and plans.\n    Has read-only filesystem access and web search capability.\n\n    Implements ReAct loop: Agent can call tools (e.g., read_file) and\n    receive results back to inform its final plan.\n    \"\"\"\n\n    async def process(self, context: Dict) -> Dict:\n        req_node = context['nodes'][0]\n\n        # Check for escalation context (from feedback controller)\n        escalation_context = req_node.get('escalation_context')\n        template_vars = {'req_id': req_node.get('id', 'unknown')}\n\n        if escalation_context:\n            # This is a re-planning after failures - use escalation prompt\n            system_prompt = self._hydrate_prompt(\"architect_core_v1\", template_vars)\n            user_prompt = self._build_escalation_prompt(req_node, escalation_context)\n            self.logger.warning(f\"Processing escalated REQ {req_node['id']} with strategy change\")\n        else:\n            # Normal first-time planning\n            system_prompt = self._hydrate_prompt(\"architect_core_v1\", template_vars)\n            user_prompt = f\"\"\"REQUIREMENT: {req_node['content']}\n\nINSTRUCTIONS:\n1. If the requirement references specific files (e.g., \"inherit from X in file Y\"), use the read_file tool to examine those files FIRST.\n2. After gathering necessary context, decompose into Atomic Specs and Plans.\n3. When you have enough information, output your final plan as JSON (no tool calls).\n\nAvailable tools: read_file, list_directory, fetch_url, search_web\"\"\"\n\n        # Get filtered tools for this role\n        tools_schema = self.get_tools_schema()\n\n        # === ReAct Loop: Iterate until LLM produces final output (no tool calls) ===\n        for iteration in range(MAX_REACT_ITERATIONS):\n            self.logger.info(f\"Architect ReAct iteration {iteration + 1}/{MAX_REACT_ITERATIONS}\")\n\n            raw_response = self.gateway.call_model(\n                role=\"ARCHITECT\",\n                system_prompt=system_prompt,\n                user_context=user_prompt,\n                tools=tools_schema if tools_schema else None\n            )\n\n            # Try to parse as JSON to check for tool calls\n            try:\n                parsed = json.loads(raw_response)\n\n                if 'tool_calls' in parsed and parsed['tool_calls']:\n                    # Execute tools and get results\n                    tool_results = await self.execute_tool_calls(parsed)\n                    self.logger.info(f\"Architect tool calls executed, feeding results back to LLM\")\n\n                    # CRITICAL: Append tool results to user_prompt for next iteration\n                    user_prompt += f\"\\n\\n[TOOL RESULTS from iteration {iteration + 1}]:\\n{tool_results}\"\n\n                    # If there's partial content alongside tool calls, note it\n                    if parsed.get('content'):\n                        user_prompt += f\"\\n\\n[YOUR PREVIOUS THOUGHTS]:\\n{parsed['content']}\"\n\n                    # Continue loop - let LLM see the tool results\n                    continue\n\n                # No tool calls - this is the final response\n                # Check if it has the expected structure (new_nodes, new_edges)\n                if 'new_nodes' in parsed or 'content' in parsed:\n                    self.logger.info(f\"Architect produced final plan after {iteration + 1} iterations\")\n                    return parsed\n\n            except json.JSONDecodeError:\n                # Response is not JSON - might be raw text plan\n                pass\n\n            # If we get here, response is either non-JSON or doesn't have tool_calls\n            # Try to parse it as the final response\n            self.logger.info(f\"Architect produced final response after {iteration + 1} iterations\")\n            return self._parse_json_response(raw_response)\n\n        # Max iterations reached - return whatever we have\n        self.logger.warning(f\"Architect reached max iterations ({MAX_REACT_ITERATIONS}), returning last response\")\n        return self._parse_json_response(raw_response)\n\n    def _build_escalation_prompt(self, req_node: Dict, escalation_context: str) -> str:\n        \"\"\"\n        Build prompt for re-planning after failures.\n        Includes failure analysis and strategy change instructions.\n        \"\"\"\n        # Analyze failure patterns from escalation context\n        failure_hints = []\n\n        if \"Missing import\" in escalation_context or \"import\" in escalation_context.lower():\n            failure_hints.append(\"- Add explicit dependency specifications for all external libraries\")\n            failure_hints.append(\"- Include setup/installation instructions in the spec\")\n\n        if \"Type mismatch\" in escalation_context or \"type\" in escalation_context.lower():\n            failure_hints.append(\"- Add explicit type annotations to the specification\")\n            failure_hints.append(\"- Specify input/output types clearly\")\n\n        if \"incomplete\" in escalation_context.lower():\n            failure_hints.append(\"- Break this requirement into smaller, more atomic specifications\")\n            failure_hints.append(\"- Create sub-tasks for each major component\")\n\n        if \"complex\" in escalation_context.lower():\n            failure_hints.append(\"- Simplify the approach - prefer simpler implementation strategies\")\n            failure_hints.append(\"- Reduce dependencies and coupling\")\n\n        hints_text = \"\\n\".join(failure_hints) if failure_hints else \"- Consider a fundamentally different approach\"\n\n        return f\"\"\"\n{escalation_context}\n\nSTRATEGY CHANGE REQUIRED:\nPrevious specifications led to implementation failures. You must re-plan with a DIFFERENT approach.\n\nSuggested adjustments based on failure analysis:\n{hints_text}\n\nREQUIREMENT: {req_node['content']}\n\nCreate NEW specifications that address the root causes of the previous failures.\nDO NOT simply reproduce the failed specification - change the approach.\n\"\"\""}}, {"id": "class_RealBuilder", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealBuilder(BaseAgent):\n    \"\"\"\n    Implements code nodes from specs.\n    Has read/write files", "color": "#9B59B6", "metadata": {"name": "RealBuilder", "lineno": 142, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealBuilder(BaseAgent):\n    \"\"\"\n    Implements code nodes from specs.\n    Has read/write filesystem access.\n\n    Implements ReAct loop: Builder can read existing files to understand\n    interfaces, patterns, and dependencies before generating code.\n    \"\"\"\n\n    async def process(self, context: Dict) -> Dict:\n        spec_content = context.get('nodes', [{}])[0].get('content', \"No Spec Found\")\n        system_prompt = self._hydrate_prompt(\"builder_core_v1\", {\n            \"target_node_id\": \"CURRENT_TASK\",\n            \"target_node_spec\": spec_content,\n            \"language\": \"python\",\n            \"file_path\": \"generated_module.py\"\n        })\n\n        # Get filtered tools for this role\n        tools_schema = self.get_tools_schema()\n\n        user_prompt = f\"\"\"SPECIFICATION:\n{spec_content}\n\nINSTRUCTIONS:\n1. If the spec references existing files, base classes, or interfaces, use read_file to examine them FIRST.\n2. Use list_directory if you need to understand the project structure.\n3. After gathering context, implement the code according to the spec.\n\nCRITICAL: You MUST respond with ONLY valid JSON. No explanations outside JSON.\n\nREQUIRED OUTPUT FORMAT:\n```json\n{{\"content\": \"<your complete Python code here>\", \"metadata\": {{\"language\": \"python\", \"file_path\": \"path/to/file.py\"}}}}\n```\n\nImplement now and output JSON:\"\"\"\n\n        # === ReAct Loop: Iterate until LLM produces final code (no tool calls) ===\n        for iteration in range(MAX_REACT_ITERATIONS):\n            self.logger.info(f\"Builder ReAct iteration {iteration + 1}/{MAX_REACT_ITERATIONS}\")\n\n            raw_response = self.gateway.call_model(\n                role=\"BUILDER\",\n                system_prompt=system_prompt,\n                user_context=user_prompt,\n                tools=tools_schema if tools_schema else None\n            )\n\n            # Try to parse as JSON to check for tool calls\n            try:\n                parsed = json.loads(raw_response)\n\n                if 'tool_calls' in parsed and parsed['tool_calls']:\n                    # Execute tools and get results\n                    tool_results = await self.execute_tool_calls(parsed)\n                    self.logger.info(f\"Builder tool calls executed, feeding results back to LLM\")\n\n                    # CRITICAL: Append tool results to user_prompt for next iteration\n                    user_prompt += f\"\\n\\n[TOOL RESULTS from iteration {iteration + 1}]:\\n{tool_results}\"\n\n                    # If there's partial content alongside tool calls, note it\n                    if parsed.get('content'):\n                        user_prompt += f\"\\n\\n[YOUR PREVIOUS THOUGHTS]:\\n{parsed['content']}\"\n\n                    # Continue loop - let LLM see the tool results\n                    continue\n\n                # No tool calls - this is the final response\n                if 'content' in parsed:\n                    self.logger.info(f\"Builder produced final code after {iteration + 1} iterations\")\n                    result = parsed\n                    result['type'] = NodeType.CODE.value\n                    result['status'] = NodeStatus.PENDING.value\n                    return result\n\n            except json.JSONDecodeError:\n                # Response is not JSON - might be raw code\n                pass\n\n            # If we get here, response is either non-JSON or doesn't have tool_calls\n            self.logger.info(f\"Builder produced final response after {iteration + 1} iterations\")\n            result = self._parse_json_response(raw_response)\n            result['type'] = NodeType.CODE.value\n            result['status'] = NodeStatus.PENDING.value\n            return result\n\n        # Max iterations reached - return whatever we have\n        self.logger.warning(f\"Builder reached max iterations ({MAX_REACT_ITERATIONS}), returning last response\")\n        result = self._parse_json_response(raw_response)\n        result['type'] = NodeType.CODE.value\n        result['status'] = NodeStatus.PENDING.value\n        return result"}}, {"id": "class_RealVerifier", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealVerifier(BaseAgent):\n    \"\"\"\n    Reviews and verifies code nodes.\n    Has read-only filesy", "color": "#9B59B6", "metadata": {"name": "RealVerifier", "lineno": 237, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealVerifier(BaseAgent):\n    \"\"\"\n    Reviews and verifies code nodes.\n    Has read-only filesystem access.\n\n    Implements ReAct loop: Verifier can read referenced files to check\n    that the code correctly implements interfaces and follows patterns.\n    \"\"\"\n\n    async def process(self, context: Dict) -> Dict:\n        code_node = context['nodes'][0]\n        system_prompt = self._hydrate_prompt(\"verifier_core_v1\", {\n            \"builder_id\": \"Unknown_Builder\",\n            \"spec_id\": \"Linked_Spec\"\n        })\n\n        # Get filtered tools for this role\n        tools_schema = self.get_tools_schema()\n\n        user_prompt = f\"\"\"CODE TO VERIFY:\n```python\n{code_node['content']}\n```\n\nVERIFICATION INSTRUCTIONS:\n1. If the code imports from or inherits from other project files, use read_file to examine those files.\n2. Verify that method signatures match any base classes or interfaces.\n3. Check for common issues: missing imports, incorrect API usage, security vulnerabilities.\n4. After thorough review, output your verdict.\n\nCRITICAL: You MUST respond with ONLY valid JSON. No explanations, no questions.\nEven if the code is incomplete, respond with a verdict.\n\nREQUIRED OUTPUT FORMAT (JSON only):\n```json\n{{\"verdict\": \"PASS\", \"critique\": []}}\n```\nOR\n```json\n{{\"verdict\": \"FAIL\", \"critique\": [\"issue 1\", \"issue 2\"]}}\n```\n\nOutput your JSON verdict now:\"\"\"\n\n        # === ReAct Loop: Iterate until LLM produces final verdict (no tool calls) ===\n        for iteration in range(MAX_REACT_ITERATIONS):\n            self.logger.info(f\"Verifier ReAct iteration {iteration + 1}/{MAX_REACT_ITERATIONS}\")\n\n            raw_response = self.gateway.call_model(\n                role=\"VERIFIER\",\n                system_prompt=system_prompt,\n                user_context=user_prompt,\n                tools=tools_schema if tools_schema else None\n            )\n\n            # Try to parse as JSON to check for tool calls\n            try:\n                parsed = json.loads(raw_response)\n\n                if 'tool_calls' in parsed and parsed['tool_calls']:\n                    # Execute tools and get results\n                    tool_results = await self.execute_tool_calls(parsed)\n                    self.logger.info(f\"Verifier tool calls executed, feeding results back to LLM\")\n\n                    # CRITICAL: Append tool results to user_prompt for next iteration\n                    user_prompt += f\"\\n\\n[TOOL RESULTS from iteration {iteration + 1}]:\\n{tool_results}\"\n\n                    # If there's partial content alongside tool calls, note it\n                    if parsed.get('content'):\n                        user_prompt += f\"\\n\\n[YOUR ANALYSIS SO FAR]:\\n{parsed['content']}\"\n\n                    # Continue loop - let LLM see the tool results\n                    continue\n\n                # No tool calls - this is the final verdict\n                if 'verdict' in parsed:\n                    self.logger.info(f\"Verifier produced final verdict after {iteration + 1} iterations: {parsed.get('verdict')}\")\n                    return parsed\n\n            except json.JSONDecodeError:\n                # Response is not JSON\n                pass\n\n            # If we get here, response is either non-JSON or doesn't have tool_calls\n            self.logger.info(f\"Verifier produced final response after {iteration + 1} iterations\")\n            return self._parse_json_response(raw_response)\n\n        # Max iterations reached - return whatever we have\n        self.logger.warning(f\"Verifier reached max iterations ({MAX_REACT_ITERATIONS}), returning last response\")\n        return self._parse_json_response(raw_response)"}}, {"id": "class_RealSocrates", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealSocrates(BaseAgent):\n    \"\"\"\n    Researches and resolves ambiguity in requirements.\n    Ha", "color": "#9B59B6", "metadata": {"name": "RealSocrates", "lineno": 329, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealSocrates(BaseAgent):\n    \"\"\"\n    Researches and resolves ambiguity in requirements.\n    Has filesystem and web search access.\n\n    Implements ReAct loop: Socrates can research standards, read existing\n    code patterns, and gather context to ask better clarifying questions.\n    \"\"\"\n\n    async def process(self, context: Dict) -> Dict:\n        question = context.get('question', \"What needs clarification?\")\n        parent_req = context.get('parent_requirement', \"\")\n\n        system_prompt = \"\"\"You are Socrates, the philosophical questioner.\nYour role is to identify ambiguity and gaps in requirements.\nAsk probing questions to clarify intent.\nSearch for relevant standards and best practices.\"\"\"\n\n        user_prompt = f\"\"\"REQUIREMENT: {parent_req}\nQUESTION: {question}\n\nINSTRUCTIONS:\n1. Use read_file to examine any existing code relevant to this requirement.\n2. Use search_web or fetch_url to research standards and best practices.\n3. After gathering context, identify what is unclear and propose specific questions.\n4. Output your final response as JSON with 'questions' (list) and 'research' (summary).\n\nIdentify what is unclear and propose specific questions to resolve ambiguity.\"\"\"\n\n        tools_schema = self.get_tools_schema()\n        accumulated_research = []\n\n        # === ReAct Loop: Iterate until LLM produces final questions (no tool calls) ===\n        for iteration in range(MAX_REACT_ITERATIONS):\n            self.logger.info(f\"Socrates ReAct iteration {iteration + 1}/{MAX_REACT_ITERATIONS}\")\n\n            raw_response = self.gateway.call_model(\n                role=\"SOCRATES\",\n                system_prompt=system_prompt,\n                user_context=user_prompt,\n                tools=tools_schema if tools_schema else None\n            )\n\n            # Try to parse as JSON to check for tool calls\n            try:\n                parsed = json.loads(raw_response)\n\n                if 'tool_calls' in parsed and parsed['tool_calls']:\n                    # Execute tools and get results\n                    tool_results = await self.execute_tool_calls(parsed)\n                    accumulated_research.append(tool_results)\n                    self.logger.info(f\"Socrates tool calls executed, feeding results back to LLM\")\n\n                    # CRITICAL: Append tool results to user_prompt for next iteration\n                    user_prompt += f\"\\n\\n[RESEARCH RESULTS from iteration {iteration + 1}]:\\n{tool_results}\"\n\n                    # If there's partial content alongside tool calls, note it\n                    if parsed.get('content'):\n                        user_prompt += f\"\\n\\n[YOUR THOUGHTS SO FAR]:\\n{parsed['content']}\"\n\n                    # Continue loop - let LLM see the tool results\n                    continue\n\n                # No tool calls - this is the final response\n                if 'questions' in parsed:\n                    self.logger.info(f\"Socrates produced final questions after {iteration + 1} iterations\")\n                    # Include accumulated research\n                    if accumulated_research and not parsed.get('research'):\n                        parsed['research'] = \"\\n\".join(accumulated_research)\n                    return parsed\n\n            except json.JSONDecodeError:\n                # Response is not JSON\n                pass\n\n            # If we get here, response is either non-JSON or doesn't have tool_calls\n            self.logger.info(f\"Socrates produced final response after {iteration + 1} iterations\")\n            return {\n                \"questions\": raw_response,\n                \"research\": \"\\n\".join(accumulated_research) if accumulated_research else None\n            }\n\n        # Max iterations reached - return whatever we have\n        self.logger.warning(f\"Socrates reached max iterations ({MAX_REACT_ITERATIONS}), returning last response\")\n        return {\n            \"questions\": raw_response,\n            \"research\": \"\\n\".join(accumulated_research) if accumulated_research else None\n        }"}}, {"id": "function__build_escalation_prompt", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _build_escalation_prompt(self, req_node: Dict, escalation_context: str) -> str:\n        \"\"\"\n    ", "color": "#3498DB", "metadata": {"name": "_build_escalation_prompt", "lineno": 100, "col_offset": 4, "is_async": false, "args": ["self", "req_node", "escalation_context"], "content": "def _build_escalation_prompt(self, req_node: Dict, escalation_context: str) -> str:\n        \"\"\"\n        Build prompt for re-planning after failures.\n        Includes failure analysis and strategy change instructions.\n        \"\"\"\n        # Analyze failure patterns from escalation context\n        failure_hints = []\n\n        if \"Missing import\" in escalation_context or \"import\" in escalation_context.lower():\n            failure_hints.append(\"- Add explicit dependency specifications for all external libraries\")\n            failure_hints.append(\"- Include setup/installation instructions in the spec\")\n\n        if \"Type mismatch\" in escalation_context or \"type\" in escalation_context.lower():\n            failure_hints.append(\"- Add explicit type annotations to the specification\")\n            failure_hints.append(\"- Specify input/output types clearly\")\n\n        if \"incomplete\" in escalation_context.lower():\n            failure_hints.append(\"- Break this requirement into smaller, more atomic specifications\")\n            failure_hints.append(\"- Create sub-tasks for each major component\")\n\n        if \"complex\" in escalation_context.lower():\n            failure_hints.append(\"- Simplify the approach - prefer simpler implementation strategies\")\n            failure_hints.append(\"- Reduce dependencies and coupling\")\n\n        hints_text = \"\\n\".join(failure_hints) if failure_hints else \"- Consider a fundamentally different approach\"\n\n        return f\"\"\"\n{escalation_context}\n\nSTRATEGY CHANGE REQUIRED:\nPrevious specifications led to implementation failures. You must re-plan with a DIFFERENT approach.\n\nSuggested adjustments based on failure analysis:\n{hints_text}\n\nREQUIREMENT: {req_node['content']}\n\nCreate NEW specifications that address the root causes of the previous failures.\nDO NOT simply reproduce the failed specification - change the approach.\n\"\"\""}}, {"id": "call_range", "type": "CALL", "status": "COMPLETE", "content_preview": "range", "color": "#E67E22", "metadata": {"lineno": 362, "col_offset": 25, "args_count": 1}}, {"id": "call__parse_json_response", "type": "CALL", "status": "COMPLETE", "content_preview": "_parse_json_response", "color": "#E67E22", "metadata": {"lineno": 322, "col_offset": 19, "args_count": 1}}, {"id": "call__hydrate_prompt", "type": "CALL", "status": "COMPLETE", "content_preview": "_hydrate_prompt", "color": "#E67E22", "metadata": {"lineno": 38, "col_offset": 28, "args_count": 2}}, {"id": "call__build_escalation_prompt", "type": "CALL", "status": "COMPLETE", "content_preview": "_build_escalation_prompt", "color": "#E67E22", "metadata": {"lineno": 34, "col_offset": 26, "args_count": 2}}, {"id": "call_lower", "type": "CALL", "status": "COMPLETE", "content_preview": "lower", "color": "#E67E22", "metadata": {"lineno": 112, "col_offset": 62, "args_count": 0}}, {"id": "call_execute_tool_calls", "type": "CALL", "status": "COMPLETE", "content_preview": "execute_tool_calls", "color": "#E67E22", "metadata": {"lineno": 378, "col_offset": 41, "args_count": 1}}, {"id": "code_-4982630142759804774", "type": "CODE", "status": "PENDING", "content_preview": "\"\"\"\nGOVERNANCE AGENTS\nThe 'Police' layer of the Swarm.\n\"\"\"\nfrom typing import Dict, List\nimport netw", "color": "#96CEB4", "metadata": {"file_path": "agents/governance.py", "language": "python", "imports": ["typing", "networkx", "agents.base_agent", "core.ontology"], "classes": ["RealTreasurer", "RealSentinel", "RealCurator", "RealLibrarian"], "functions": ["process", "_generate_recommendations"]}}, {"id": "phantom_import_networkx", "type": "CODE", "status": "COMPLETE", "content_preview": "networkx", "color": "#96CEB4", "metadata": {}}, {"id": "class_RealTreasurer", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealTreasurer(BaseAgent):\n    \"\"\"\n    Monitors Cost Ledger. Enforces Prime Directive #8.\n    \"", "color": "#9B59B6", "metadata": {"name": "RealTreasurer", "lineno": 10, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealTreasurer(BaseAgent):\n    \"\"\"\n    Monitors Cost Ledger. Enforces Prime Directive #8.\n    \"\"\"\n    async def process(self, context: Dict) -> Dict:\n        # 1. Calculate current spend\n        current_spend = self.gateway._cost_session\n        budget = self.gateway.config['cost_limits']['project_total_limit_usd']\n\n        # 2. Enforce Limits\n        if current_spend > budget:\n            return {\n                \"verdict\": \"HALT\",\n                \"reason\": f\"Budget exceeded: ${current_spend} > ${budget}\"\n            }\n\n        # 3. Forecast (Simple Heuristic)\n        remaining = budget - current_spend\n        return {\n            \"verdict\": \"APPROVE\",\n            \"remaining_budget\": remaining,\n            \"status\": \"SOLVENT\"\n        }"}}, {"id": "class_RealSentinel", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealSentinel(BaseAgent):\n    \"\"\"\n    Monitors Data Flow. Enforces Security.\n    \"\"\"\n    async ", "color": "#9B59B6", "metadata": {"name": "RealSentinel", "lineno": 34, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealSentinel(BaseAgent):\n    \"\"\"\n    Monitors Data Flow. Enforces Security.\n    \"\"\"\n    async def process(self, context: Dict) -> Dict:\n        code_content = context.get('nodes', [{}])[0].get('content', \"\")\n\n        # 1. Static Analysis (Taint Checking stub)\n        suspicious_patterns = [\"eval(\", \"exec(\", \"subprocess.call(shell=True)\"]\n        issues = [p for p in suspicious_patterns if p in code_content]\n\n        if issues:\n            return {\n                \"verdict\": \"REJECT\",\n                \"security_issues\": issues,\n                \"action\": \"BLOCK_NODE\"\n            }\n\n        return {\"verdict\": \"SAFE\", \"scan_depth\": \"static_only\"}"}}, {"id": "class_RealCurator", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealCurator(BaseAgent):\n    \"\"\"\n    Manages Graph Integrity. Enforces Prime Directive #11-13.\n", "color": "#9B59B6", "metadata": {"name": "RealCurator", "lineno": 55, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealCurator(BaseAgent):\n    \"\"\"\n    Manages Graph Integrity. Enforces Prime Directive #11-13.\n    - Garbage collection of DEAD_END nodes\n    - Orphan detection\n    - Citation validation\n    \"\"\"\n    async def process(self, context: Dict) -> Dict:\n        graph = self.graph_db.graph\n        issues = []\n        actions_taken = []\n\n        # 1. Find orphan nodes (no incoming edges, not REQ type)\n        orphans = []\n        for node in graph.nodes():\n            if graph.in_degree(node) == 0:\n                node_type = graph.nodes[node].get('type')\n                if node_type != NodeType.REQ.value:\n                    orphans.append(node)\n\n        if orphans:\n            issues.append(f\"Found {len(orphans)} orphan nodes\")\n\n        # 2. Find nodes missing TRACES_TO edges\n        missing_traceability = []\n        for node in graph.nodes():\n            has_trace = any(\n                d.get('type') == EdgeType.TRACES_TO.value\n                for _, _, d in graph.out_edges(node, data=True)\n            )\n            node_type = graph.nodes[node].get('type')\n            if not has_trace and node_type not in [NodeType.REQ.value, NodeType.STATE.value]:\n                missing_traceability.append(node)\n\n        if missing_traceability:\n            issues.append(f\"Found {len(missing_traceability)} nodes missing traceability\")\n\n        # 3. Prune DEAD_END nodes\n        dead_ends = [\n            n for n, d in graph.nodes(data=True)\n            if d.get('type') == NodeType.DEAD_END.value\n        ]\n        if dead_ends:\n            self.graph_db.prune_dead_ends()\n            actions_taken.append(f\"Pruned {len(dead_ends)} dead-end nodes\")\n\n        # 4. Check for cycles (should never happen due to add_edge guard)\n        try:\n            cycles = list(nx.simple_cycles(graph))\n            if cycles:\n                issues.append(f\"CRITICAL: {len(cycles)} cycles detected!\")\n        except:\n            pass\n\n        return {\n            \"verdict\": \"HEALTHY\" if not issues else \"NEEDS_ATTENTION\",\n            \"issues\": issues,\n            \"actions_taken\": actions_taken,\n            \"node_count\": graph.number_of_nodes(),\n            \"edge_count\": graph.number_of_edges()\n        }"}}, {"id": "class_RealLibrarian", "type": "CLASS", "status": "COMPLETE", "content_preview": "class RealLibrarian(BaseAgent):\n    \"\"\"\n    Manages System Prompts & Optimization. Enforces prompt q", "color": "#9B59B6", "metadata": {"name": "RealLibrarian", "lineno": 118, "col_offset": 0, "bases": ["BaseAgent"], "content": "class RealLibrarian(BaseAgent):\n    \"\"\"\n    Manages System Prompts & Optimization. Enforces prompt quality.\n    - Tracks prompt effectiveness\n    - Suggests optimizations\n    \"\"\"\n    async def process(self, context: Dict) -> Dict:\n        # Analyze prompt templates\n        templates = self.templates\n        analysis = {}\n\n        for template_name, template_data in templates.items():\n            if isinstance(template_data, dict) and 'instruction' in template_data:\n                instruction = template_data['instruction']\n                analysis[template_name] = {\n                    \"char_count\": len(instruction),\n                    \"estimated_tokens\": len(instruction) // 4,\n                    \"has_output_schema\": 'output_schema' in template_data,\n                    \"placeholder_count\": instruction.count('{')\n                }\n\n        # Calculate total prompt budget\n        total_tokens = sum(a['estimated_tokens'] for a in analysis.values())\n\n        return {\n            \"verdict\": \"OPTIMIZED\" if total_tokens < 2000 else \"REVIEW_NEEDED\",\n            \"template_analysis\": analysis,\n            \"total_estimated_tokens\": total_tokens,\n            \"recommendations\": self._generate_recommendations(analysis)\n        }\n\n    def _generate_recommendations(self, analysis: Dict) -> List[str]:\n        recommendations = []\n        for name, data in analysis.items():\n            if data['estimated_tokens'] > 500:\n                recommendations.append(f\"Consider shortening {name} (currently ~{data['estimated_tokens']} tokens)\")\n            if not data['has_output_schema']:\n                recommendations.append(f\"Add output_schema to {name} for structured responses\")\n        return recommendations"}}, {"id": "function__generate_recommendations", "type": "FUNCTION", "status": "COMPLETE", "content_preview": "def _generate_recommendations(self, analysis: Dict) -> List[str]:\n        recommendations = []\n     ", "color": "#3498DB", "metadata": {"name": "_generate_recommendations", "lineno": 149, "col_offset": 4, "is_async": false, "args": ["self", "analysis"], "content": "def _generate_recommendations(self, analysis: Dict) -> List[str]:\n        recommendations = []\n        for name, data in analysis.items():\n            if data['estimated_tokens'] > 500:\n                recommendations.append(f\"Consider shortening {name} (currently ~{data['estimated_tokens']} tokens)\")\n            if not data['has_output_schema']:\n                recommendations.append(f\"Add output_schema to {name} for structured responses\")\n        return recommendations"}}, {"id": "call_items", "type": "CALL", "status": "COMPLETE", "content_preview": "items", "color": "#E67E22", "metadata": {"lineno": 151, "col_offset": 26, "args_count": 0}}, {"id": "call_sum", "type": "CALL", "status": "COMPLETE", "content_preview": "sum", "color": "#E67E22", "metadata": {"lineno": 140, "col_offset": 23, "args_count": 1}}, {"id": "call_prune_dead_ends", "type": "CALL", "status": "COMPLETE", "content_preview": "prune_dead_ends", "color": "#E67E22", "metadata": {"lineno": 98, "col_offset": 12, "args_count": 0}}, {"id": "call_list", "type": "CALL", "status": "COMPLETE", "content_preview": "list", "color": "#E67E22", "metadata": {"lineno": 103, "col_offset": 21, "args_count": 1}}, {"id": "call_number_of_nodes", "type": "CALL", "status": "COMPLETE", "content_preview": "number_of_nodes", "color": "#E67E22", "metadata": {"lineno": 113, "col_offset": 26, "args_count": 0}}, {"id": "call_number_of_edges", "type": "CALL", "status": "COMPLETE", "content_preview": "number_of_edges", "color": "#E67E22", "metadata": {"lineno": 114, "col_offset": 26, "args_count": 0}}, {"id": "call__generate_recommendations", "type": "CALL", "status": "COMPLETE", "content_preview": "_generate_recommendations", "color": "#E67E22", "metadata": {"lineno": 146, "col_offset": 31, "args_count": 1}}, {"id": "call_in_degree", "type": "CALL", "status": "COMPLETE", "content_preview": "in_degree", "color": "#E67E22", "metadata": {"lineno": 70, "col_offset": 15, "args_count": 1}}, {"id": "call_simple_cycles", "type": "CALL", "status": "COMPLETE", "content_preview": "simple_cycles", "color": "#E67E22", "metadata": {"lineno": 103, "col_offset": 26, "args_count": 1}}, {"id": "call_len", "type": "CALL", "status": "COMPLETE", "content_preview": "len", "color": "#E67E22", "metadata": {"lineno": 105, "col_offset": 43, "args_count": 1}}, {"id": "call_values", "type": "CALL", "status": "COMPLETE", "content_preview": "values", "color": "#E67E22", "metadata": {"lineno": 140, "col_offset": 58, "args_count": 0}}], "links": [{"source": "code_-3305019123490435745", "target": "phantom_import_json", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "phantom_import_uuid", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "phantom_import_typing", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "phantom_import_agents.base_agent", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "phantom_import_core.ontology", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "phantom_import_infrastructure.graph_db", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "class_RealTestGenerator", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "function_generate_tests_for_verified_code", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "function_process", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "function__build_system_prompt", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "function__build_user_prompt", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "function__parse_test_response", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "function__generate_test_path", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_nodes", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_get", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call__build_system_prompt", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call__build_user_prompt", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_get_tools_schema", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_call_model", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call__parse_test_response", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_any", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_out_edges", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call__generate_test_path", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_find", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_strip", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_count", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_replace", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_add_node", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_add_edge", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_append", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_join", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_process", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_print", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_predecessors", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-3305019123490435745", "target": "call_uuid4", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_json", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_logging", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_yaml", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_os", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_time", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_abc", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_typing", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_cryptography.hazmat.primitives.asymmetric", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_cryptography.hazmat.primitives", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_infrastructure.llm_gateway", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_core.ontology", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_infrastructure.mcp_hub", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "phantom_import_re", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "class_BaseAgent", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function___init__", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function__save_keys", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_acquire_lock", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_release_lock", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_sign_content", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function__hydrate_prompt", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function__parse_json_response", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_get_tools_schema", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_check_tool_permission", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_execute_tool_calls", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "function_process", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_LLMGateway", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_getLogger", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_generate", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_public_key", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call__save_keys", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_get", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_makedirs", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_public_bytes", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_time", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_exists", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_encode", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_hex", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_format", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_strip", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_search", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_print", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_warning", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_join", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_open", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_safe_load", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_read", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_write", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_remove", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_ValueError", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_loads", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_get_tools_for_role", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_isinstance", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_dumps", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_sign", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_check_tool_permission", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_append", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_info", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_sleep", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_group", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_execute_tool", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_getmtime", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-6102688374110437732", "target": "call_str", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "phantom_import_json", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "phantom_import_typing", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "phantom_import_agents.base_agent", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "phantom_import_core.ontology", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "class_RealArchitect", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "class_RealBuilder", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "class_RealVerifier", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "class_RealSocrates", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "function_process", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "function__build_escalation_prompt", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_get", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_get_tools_schema", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_range", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_warning", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call__parse_json_response", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call__hydrate_prompt", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call__build_escalation_prompt", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_info", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_call_model", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_append", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_lower", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_join", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_loads", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_1449790842580194339", "target": "call_execute_tool_calls", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "phantom_import_typing", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "phantom_import_networkx", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "phantom_import_agents.base_agent", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "phantom_import_core.ontology", "type": "DEPENDS_ON", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "class_RealTreasurer", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "class_RealSentinel", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "class_RealCurator", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "class_RealLibrarian", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "function_process", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "function__generate_recommendations", "type": "CONTAINS", "color": "#2ECC71", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_get", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_nodes", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_items", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_sum", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_append", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_any", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_prune_dead_ends", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_list", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_number_of_nodes", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_number_of_edges", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call__generate_recommendations", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_in_degree", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_simple_cycles", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_isinstance", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_len", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_count", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_values", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}, {"source": "code_-4982630142759804774", "target": "call_out_edges", "type": "REFERENCES", "color": "#E74C3C", "signed_by": null, "created_at": null}]};

        const width = window.innerWidth;
        const height = window.innerHeight;

        const svg = d3.select("#graph")
            .append("svg")
            .attr("width", width)
            .attr("height", height);

        const simulation = d3.forceSimulation(data.nodes)
            .force("link", d3.forceLink(data.links).id(d => d.id).distance(100))
            .force("charge", d3.forceManyBody().strength(-300))
            .force("center", d3.forceCenter(width / 2, height / 2));

        const link = svg.append("g")
            .selectAll("line")
            .data(data.links)
            .join("line")
            .attr("class", "link")
            .attr("stroke", d => d.color)
            .attr("stroke-width", 2);

        const node = svg.append("g")
            .selectAll("g")
            .data(data.nodes)
            .join("g")
            .attr("class", "node")
            .call(d3.drag()
                .on("start", dragstarted)
                .on("drag", dragged)
                .on("end", dragended));

        node.append("circle")
            .attr("r", 20)
            .attr("fill", d => d.color);

        node.append("text")
            .attr("dy", 4)
            .attr("text-anchor", "middle")
            .text(d => d.type);

        node.append("title")
            .text(d => d.id + "\n" + d.content_preview);

        simulation.on("tick", () => {
            link
                .attr("x1", d => d.source.x)
                .attr("y1", d => d.source.y)
                .attr("x2", d => d.target.x)
                .attr("y2", d => d.target.y);
            node.attr("transform", d => `translate(${d.x},${d.y})`);
        });

        function dragstarted(event) {
            if (!event.active) simulation.alphaTarget(0.3).restart();
            event.subject.fx = event.subject.x;
            event.subject.fy = event.subject.y;
        }

        function dragged(event) {
            event.subject.fx = event.x;
            event.subject.fy = event.y;
        }

        function dragended(event) {
            if (!event.active) simulation.alphaTarget(0);
            event.subject.fx = null;
            event.subject.fy = null;
        }
    </script>
</body>
</html>